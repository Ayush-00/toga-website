<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <meta name="description" content="TOGA">
  <meta property="og:title" content="TOGA: Temporally Grounded Open-Ended Video QA with Weak Supervision"/>
  <meta property="og:description" content="TOGA ICCV 2025"/>
  <meta property="og:url" content="YOUR_PROJECT_URL"/>
  <meta property="og:image" content="static/images/your_banner_image.png"/>
  <meta property="og:image:width" content="1200"/>
  <meta property="og:image:height" content="630"/>


  <meta name="twitter:title" content="TOGA: Temporally Grounded Open-Ended Video QA with Weak Supervision">
  <meta name="twitter:description" content="TOGA ICCV 2025">
  <meta name="twitter:image" content="static/images/your_banner_image.png">
  <meta name="twitter:card" content="summary_large_image">
  <meta name="keywords" content="Video Question Answering, Temporal Grounding, Weak Supervision, Vision-Language Models, Deep Learning">
  <meta name="viewport" content="width=device-width, initial-scale=1">


  <title>TOGA ICCV 2025</title>
  <link rel="icon" type="image/x-icon" href="static/images/your_icon.png">
  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro"
  rel="stylesheet">

  <link rel="stylesheet" href="static/css/bulma.min.css">
  <link rel="stylesheet" href="static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="static/css/fontawesome.all.min.css">
  <link rel="stylesheet"
  href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="static/css/index.css">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script src="https://documentcloud.adobe.com/view-sdk/main.js"></script>
  <script defer src="static/js/fontawesome.all.min.js"></script>
  <script src="static/js/bulma-carousel.min.js"></script>
  <script src="static/js/bulma-slider.min.js"></script>
  <script src="static/js/index.js"></script>
</head>
<body>


  <section class="hero">
    <div class="hero-body">
      <div class="container is-max-desktop">
        <div class="columns is-centered">
          <div class="column has-text-centered">
            <h1 class="title is-1 publication-title">TOGA: Temporally Grounded Open-Ended Video QA with Weak Supervision</h1>
            <div class="is-size-5 publication-authors">
              <span class="author-block">
                <a href="#" target="_blank">Ayush Gupta</a><sup>1,2</sup>,</span>
                <span class="author-block">
                  <a href="#" target="_blank">Anirban Roy</a><sup>1</sup>,</span>
                <span class="author-block">
                  <a href="#" target="_blank">Rama Chellappa</a><sup>2</sup>,
                </span>
                <span class="author-block">
                    <a href="#" target="_blank">Nathaniel D. Bastian</a><sup>3</sup>,
                </span>
                <span class="author-block">
                    <a href="#" target="_blank">Alvaro Velasquez</a><sup>1</sup>,
                </span>
                <span class="author-block">
                    <a href="#" target="_blank">Susmit Jha</a><sup>1</sup>
                </span>
            </div>

            <div class="is-size-5 publication-authors">
              <span class="author-block"><sup>1</sup>SRI, <sup>2</sup>Johns Hopkins University, <sup>3</sup>United States Military Academy<br>ICCV 2025</span>
            </div>

            <div class="column has-text-centered">
              <div class="publication-links">
                <span class="link-block">
                  <a href="#" target="_blank"
                  class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                    <i class="fab fa-github"></i>
                  </span>
                  <span>Code (Coming Soon)</span>
                </a>
              </span>

              <span class="link-block">
                <a href="https://arxiv.org/abs/2506.09445" target="_blank"
                class="external-link button is-normal is-rounded is-dark">
                <span class="icon">
                  <i class="ai ai-arxiv"></i>
                </span>
                <span>arXiv</span>
              </a>
            </span>
            </div>
          </div>
        </div>
      </div>
    </div>
  </div>
</section>


<section class="section hero is-light">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Abstract</h2>
        <div class="content has-text-justified">
          <p>
            We address the problem of video question answering (video QA) with temporal grounding in a weakly supervised setup, without any temporal annotations. Given a video and a question, we generate an open-ended answer grounded with the start and end time. For this task, we propose TOGA: a vision-language model for Temporally Grounded Open-Ended Video QA with Weak Supervision. We instruct-tune TOGA to jointly generate the answer and the temporal grounding. We operate in a weakly supervised setup where the temporal grounding annotations are not available. We generate pseudo labels for temporal grounding and ensure the validity of these labels by imposing a consistency constraint between the question of a grounding response and the response generated by a question referring to the same temporal segment. We notice that jointly generating the answers with the grounding improves performance on question answering as well as grounding. We evaluate TOGA on grounded QA and open-ended QA tasks. For grounded QA, we consider the NEXT-GQA benchmark. For open-ended QA, we consider the MSVD-QA and ActivityNet-QA benchmarks. We achieve state-of-the-art performance for both tasks on these benchmarks.
          </p>
        </div>
      </div>
    </div>
  </div>
</section>

<section class="hero is-small">
    <div class="hero-body">
      <div class="container">
        <h2 class="subtitle has-text-centered">
          TOGA generates open-ended answers and grounds them with start and end times in the video.
        </h2>
      </div>
    </div>
</section>

<section class="section">
  <div class="container is-max-desktop">
    <div class="columns is-centered">
      <div class="column is-full">
        <h2 class="title is-3 has-text-centered">Overview of TOGA</h2>
        <figure class="image">
          </figure>
      </div>
    </div>
    <div class="columns is-centered">
      <div class="column is-full">
        <p class="has-text-justified">
          Our VLM framework consists of four main modules: 1) a frozen vision encoder to compute frame-wise features, 2) a frozen large-language text encoder to compute text features from questions, 3) a multi-scale vision-language connector (MS-VLC) to align vision and text features, and 4) a large-language text decoder that we train to generate open-ended answers with grounding. The MS-VLC processes video features at two temporal resolutions: a sparse scale to capture low-frequency, long-term cues and a dense scale to capture high-frequency, short-term cues. This multi-scale approach is crucial for accurately grounding both long and short events. The language decoder is instruction-tuned to jointly generate the answer and its temporal grounding in the format: `Answer [start time, end time]`.
        </p>
      </div>
    </div>
  </div>
</section>

<section class="section">
  <div class="container is-max-desktop">
    <div class="columns is-centered">
      <div class="column is-full">
        <h2 class="title is-3 has-text-centered">Multi-Stage Weakly Supervised Training</h2>
        <figure class="image">
          </figure>
      </div>
    </div>
    <div class="columns is-centered">
      <div class="column is-full">
        <p class="has-text-justified">
            Since ground-truth temporal annotations are expensive and often unavailable, we train TOGA in a weakly supervised, multi-stage process.
            <br><b>1. Vision-Text Alignment:</b> First, we train only the MS-VLC module to align the video and text features using diverse data like video captions and general QA pairs.
            <br><b>2. Instruction Tuning with Pseudo-Labels:</b> Next, we train the model to understand and generate temporal groundings. We create pseudo-labels by taking video segments, generating descriptions for them, and using these descriptions and segment times as noisy training data.
            <br><b>3. Consistency Constraint:</b> Finally, to refine grounding accuracy, we enforce a consistency constraint. For a generated answer like `The boy is running [10, 20]`, we create a corresponding question, `What is happening in [10, 20]?`, and train the model to produce a consistent answer (`A boy in a red shirt is running`). This self-consistency helps the model learn to ground answers accurately without explicit labels.
        </p>
      </div>
    </div>
  </div>
</section>


<section class="hero is-small">
  <div class="hero-body">
    <div class="container">
      <h2 class="title is-3 has-text-centered">Evaluation</h2>
      <div class="content has-text-justified">
        <p>
          We evaluate TOGA on challenging benchmarks for both grounded and open-ended video QA. For <b>grounded video QA</b>, we use NEXT-GQA and ReXTime, which feature long videos with complex causal and temporal questions. We use metrics like Intersection over Union (IoU) and IoU@0.5 to measure grounding accuracy, and Acc@GQA, which evaluates both the correctness of the answer and the grounding. For <b>open-ended video QA</b>, we use the MSVD-QA and ActivityNet-QA datasets. Here, we measure performance by using an LLM (GPT-3.5-turbo) to compare our model's generated answer with the ground truth, calculating both an accuracy score and a quality score from 1 to 5.
        </p>
      </div>
    </div>
  </div>
</section>


<section class="section">
  <div class="container is-max-desktop">
    <div class="columns is-centered">
      <div class="column is-full">
        <h2 class="title is-3 has-text-centered">Results</h2>
        <figure class="image">
            <h2 class="subtitle has-text-centered">
              TOGA achieves state-of-the-art results on the NEXT-GQA benchmark for weakly supervised grounded video QA.
            </h2>
        </figure>
      </div>
    </div>
    <div class="columns is-centered">
      <div class="column is-full">
        <p class="has-text-justified">
           Our main results show that TOGA significantly outperforms existing methods on both grounded and open-ended QA tasks. On NEXT-GQA, TOGA sets a new state-of-the-art on all grounding metrics, even though our open-ended setup is more challenging than the multiple-choice format used by prior work. We also achieve top performance on the MSVD-QA and ActivityNet-QA benchmarks, demonstrating the effectiveness of our multi-scale architecture for general video understanding. These results highlight the benefits of jointly generating answers and groundings and leveraging a multi-scale approach to capture temporal features.
        </p>
      </div>
    </div>

    <div class="columns is-centered" style="margin-top: 2rem;">
        <div class="column is-full">
          <h3 class="title is-4 has-text-centered">Qualitative Examples</h3>
          <figure class="image">
              <h2 class="subtitle has-text-centered">
                Qualitative results on NEXT-GQA showing both causal and temporal questions.
              </h2>
          </figure>
        </div>
      </div>
  </div>
</section>


<section class="section">
  <div class="container is-max-desktop">
    <div class="columns is-centered">
      <div class="column is-full">
        <h2 class="title is-3 has-text-centered">Ablations</h2>
        <figure class="image">
            </figure>
      </div>
    </div>
    <div class="columns is-centered">
      <div class="column is-full">
        <p class="has-text-justified">
            We performed several ablation studies to validate our design choices.
            <br> • <b>Multi-Scale Connector (MS-VLC):</b> We compared our multi-scale (sparse + dense) approach to single-scale variants (sparse-only or dense-only). The results show that the MS-VLC is superior, particularly for grounding very short and very long events, confirming that capturing features at multiple temporal resolutions is critical for performance.
            <br> • <b>Consistency Constraint:</b> To measure the impact of our final training stage, we trained a model variant without the consistency constraint. This model achieved an mIoU of only 12.1 on NEXT-GQA, a significant drop from the 24.4 achieved by the full TOGA model. This demonstrates that enforcing consistency is essential for learning accurate grounding in a weakly supervised setting.
        </p>
      </div>
    </div>
  </div>
</section>


<section class="hero is-small is-light">
  <div class="hero-body">
    <div class="container">
      <h2 class="title is-3">Video Presentation (Placeholder)</h2>
      <div class="columns is-centered has-text-centered">
        <div class="column is-four-fifths">
          
          <div class="publication-video">
            <iframe width="560" height="315" src="https://www.youtube.com/embed/dQw4w9WgXcQ" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" allowfullscreen></iframe>
          </div>
        </div>
      </div>
    </div>
  </div>
</section>


<section class="hero is-small is-light">
  <div class="hero-body">
    <div class="container">
      <h2 class="title">Poster (Placeholder)</h2>

      <iframe  src="#" width="100%" height="550">
          </iframe>
        
      </div>
    </div>
  </section>
<section class="section" id="BibTeX">
    <div class="container is-max-desktop content">
      <h2 class="title">BibTeX</h2>
      <pre><code>
@inproceedings{gupta2025iccv,
  title={TOGA: Temporally Grounded Open-Ended Video QA with Weak Supervision},
  author={Gupta, Ayush and Roy, Anirban and Chellappa, Rama and Bastian, Nathaniel D. and Velasquez, Alvaro and Jha, Susmit},
  booktitle={IEEE/CVF International Conference on Computer Vision (ICCV)},
  year={2025}
}
      </code></pre>
    </div>
</section>
<footer class="footer">
  <div class="container">
    <div class="columns is-centered">
      <div class="column is-8">
        <div class="content">

          <p>
            This page was built using the <a href="https://github.com/eliahuhorwitz/Academic-project-page-template" target="_blank">Academic Project Page Template</a> which was adopted from the <a href="https://nerfies.github.io" target="_blank">Nerfies</a> project page.
          </p>

        </div>
      </div>
    </div>
  </div>
</footer>

</body>
</html>